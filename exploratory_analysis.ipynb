{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset esnli (/Users/julianbruinsma/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc)\n",
      "100%|██████████| 3/3 [00:00<00:00, 20.38it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"esnli\")\n",
    "\n",
    "# Take subset of data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Get features from the dataset\n",
    "train = pd.DataFrame.from_dict(dataset['train'])\n",
    "\n",
    "# Take first 10 examples\n",
    "df = train.iloc[:10]\n",
    "\n",
    "\n",
    "print(type(df['premise']))\n",
    "\n",
    "# from train dataframe, get the premise and hypothesis\n",
    "premise = df['premise']\n",
    "hypothesis = df['hypothesis']\n",
    "\n",
    "# print(premise)\n",
    "# print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral </s> the person is not necessarily training his horse </s>\n",
      "contradiction </s> One cannot be on a jumping horse cannot be a diner ordering food. </s>\n",
      "entailment </s> a broken down airplane is outdoors </s>\n",
      "neutral </s> Just because they are smiling and waving at a camera does not imply their parents or anyone is anyone behind it </s>\n",
      "entailment </s> The children must be present to see them smiling and waving. </s>\n",
      "contradiction </s> One cannot be smiling and frowning at the same time. </s>\n",
      "contradiction </s> One cannot be in the middle of a bridge if they are on the sidewalk. </s>\n",
      "entailment </s> jumping on skateboard is the same as doing trick on skateboard. </s>\n",
      "neutral </s> Just because the boy is jumping on a skateboard does not imply he is wearing safety equipment </s>\n",
      "neutral </s> it is not necessarily true the man drinks his juice </s>\n",
      "{'input_ids': [7163, 1, 34, 19, 59, 6539, 1176, 8, 388, 6750, 112, 5143, 1, 1], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [389, 2749, 388, 2561, 7, 28, 112, 5470, 5143, 44, 3, 9, 422, 953, 16, 3, 9, 1975, 1814, 298, 1652, 16, 2756, 11999, 3, 9955, 3993, 16, 8, 2458, 5, 1, 389, 2749, 388, 6750, 112, 5143, 38, 3, 88, 1749, 7, 21, 112, 3062, 12, 129, 326, 161, 5, 1], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "# import pad_sequence\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\", use_fast=True)\n",
    "\n",
    "# concatenate premise and hypothesis with separator token </s> \n",
    "input_ids = []\n",
    "attention_mask = []\n",
    "token_type_ids = []\n",
    "target_ids = []\n",
    "\n",
    "premise_list = df[\"premise\"].tolist()\n",
    "hypothesis_list = df[\"hypothesis\"].tolist()\n",
    "explanation_list = df[\"explanation_1\"].tolist()\n",
    "label_list = df[\"label\"].tolist()\n",
    "\n",
    "for (premise, hypothesis, explanation, label) in zip(premise_list, hypothesis_list, explanation_list, label_list):\n",
    "    premise_hypothesis = f\"{premise} </s> {hypothesis}\"\n",
    "    \n",
    "    # make switch case for label if it is 0 label = entailment, if it is 1 label = neutral, if it is 2 label = contradiction\n",
    "    if label == 0:\n",
    "        label = \"entailment\"\n",
    "    elif label == 1:\n",
    "        label = \"neutral\"\n",
    "    elif label == 2:\n",
    "        label = \"contradiction\"\n",
    "    \n",
    "    # Concatenate label with explanation\n",
    "\n",
    "    label_explanation = f\"{label} </s> {explanation} </s>\" \n",
    "    print(label_explanation)\n",
    "    \n",
    "    \n",
    "    hypothesis_premise_tokens = tokenizer.encode_plus(\n",
    "        premise_hypothesis,\n",
    "        truncation=True, \n",
    "        return_token_type_ids=True, \n",
    "        max_length=128,\n",
    "        )\n",
    "    \n",
    "    target_encoding = tokenizer.encode_plus(\n",
    "        label_explanation,\n",
    "        truncation=True,\n",
    "        padding=\"longest\",\n",
    "        return_token_type_ids=True,\n",
    "        max_length=128,\n",
    "    )\n",
    "    \n",
    "\n",
    "    token_type_ids.append(torch.Tensor(hypothesis_premise_tokens.token_type_ids))\n",
    "    attention_mask.append(torch.Tensor(hypothesis_premise_tokens.attention_mask))\n",
    "    input_ids.append(torch.Tensor(hypothesis_premise_tokens.input_ids))\n",
    "    target_ids.append(torch.Tensor(target_encoding.input_ids))\n",
    "\n",
    "token_type_ids = pad_sequence(token_type_ids, batch_first=True)\n",
    "attention_mask = pad_sequence(attention_mask, batch_first=True)\n",
    "input_ids = pad_sequence(input_ids, batch_first=True)\n",
    "target = pad_sequence(target_ids, batch_first=True)\n",
    "\n",
    "dataset = TensorDataset(\n",
    "    input_ids,\n",
    "    attention_mask,\n",
    "    token_type_ids,\n",
    "    target\n",
    ")\n",
    "\n",
    "print(target_encoding)\n",
    "print(hypothesis_premise_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  568,    19,    59,  6539,   761,   112,  4952,     1,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100],\n",
      "        [ 1178,    36,    30,     3,     9, 15539,  4952,  1178,    36,     3,\n",
      "             9,   176,    49, 12320,   542,     5,     1,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100],\n",
      "        [    9,  4335,   323, 20527,    19, 10962,     1,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100],\n",
      "        [  250,    79,    33, 20770,    11,  8036,  3745,    44,     3,     9,\n",
      "          1861,   405,    59,     3, 18531,    70,  1362,    42,  1321,    19,\n",
      "          1321,  1187,    34,     1],\n",
      "        [  502,   398,    36,   915,    12,   217,   135, 20770,    11,  8036,\n",
      "          3745,     5,     1,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100],\n",
      "        [ 1178,    36, 20770,    11,  2515,  9197,    53,    44,     8,   337,\n",
      "            97,     5,     1,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100],\n",
      "        [ 1178,    36,    16,     8,  2214,    13,     3,     9,  4716,     3,\n",
      "            99,    79,    33,    30,     8, 21393,     5,     1,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100],\n",
      "        [   30, 16573,  1976,    19,     8,   337,    38,   692,  7873,    30,\n",
      "         16573,  1976,     5,     1,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100],\n",
      "        [  250,     8,  4940,    19, 15539,    30,     3,     9, 16573,  1976,\n",
      "           405,    59,     3, 18531,     3,    88,    19,  5119,  1455,  1277,\n",
      "             1,  -100,  -100,  -100],\n",
      "        [   19,    59,  6539,  1176,     8,   388,  6750,   112,  5143,     1,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100]])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "target_device = target.to(device).long()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# get shape of target device\n",
    "# print(target_device[:, :-1].contiguous())\n",
    "# \n",
    "# \n",
    "# print(\"detaching\")\n",
    "# print(target_device[:, 1:].clone().detach())\n",
    "# print(target_device[0].clone().detach())\n",
    "\n",
    "lm_labels = target_device[:, 1:].clone().detach()\n",
    "lm_labels[target_device[:, 1:] == tokenizer.pad_token_id] = -100 \n",
    "print(lm_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset esnli (/Users/julianbruinsma/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc)\n",
      "100%|██████████| 3/3 [00:00<00:00, 75.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing data loader\n",
      "initializing train data loader\n",
      "initializing val data loader\n",
      "initializing test data loader\n",
      "<class 'torch.utils.data.dataloader.DataLoader'>\n"
     ]
    }
   ],
   "source": [
    "from dataloader import esnli\n",
    "\n",
    "print(\"initializing dataset\")\n",
    "dataset = esnli()\n",
    "print(\"initializing data loader\")\n",
    "train_loader, val_loader, test_loader = dataset.get_data_loaders()\n",
    "\n",
    "print(type(train_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog - he is a joy to walk with and is very affectionate with me. I like to spend time with him on walks with his kitty cat, leo, who is so cute!\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "input_ids = tokenizer.encode('summarize: I enjoy walking with my cute dog', return_tensors='pt')\n",
    "greedy_output = model.generate(input_ids, num_beams=7, no_repeat_ngram_size=2, min_length=50, max_length=100)\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 555, 1178,   36,  ...,    0,    0,    0],\n",
      "        [ 466,    8, 1021,  ...,    0,    0,    0],\n",
      "        [   8,  861,   19,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [  37, 4940,   19,  ...,    0,    0,    0],\n",
      "        [ 290,   19,  893,  ...,    0,    0,    0],\n",
      "        [  37, 1076,   33,  ...,    0,    0,    0]])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/julianbruinsma/stack/School/Master/Year_2/Natural Language Processing/Final Project/NLP-Final_Project/exploratory_analysis.ipynb Cell 8'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/exploratory_analysis.ipynb#ch0000008?line=32'>33</a>\u001b[0m \u001b[39m# Make sure that lm_labels are not set for 0 (pad) tokens\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/exploratory_analysis.ipynb#ch0000008?line=33'>34</a>\u001b[0m lm_labels[target_ids[:, \u001b[39m1\u001b[39m:] \u001b[39m==\u001b[39m tokenizer\u001b[39m.\u001b[39mpad_token_id] \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m100\u001b[39m \n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/exploratory_analysis.ipynb#ch0000008?line=36'>37</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/exploratory_analysis.ipynb#ch0000008?line=37'>38</a>\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/exploratory_analysis.ipynb#ch0000008?line=38'>39</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/exploratory_analysis.ipynb#ch0000008?line=39'>40</a>\u001b[0m     decoder_input_ids\u001b[39m=\u001b[39;49my_ids,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/exploratory_analysis.ipynb#ch0000008?line=40'>41</a>\u001b[0m     labels\u001b[39m=\u001b[39;49mlm_labels,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/exploratory_analysis.ipynb#ch0000008?line=41'>42</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/exploratory_analysis.ipynb#ch0000008?line=42'>43</a>\u001b[0m \u001b[39m# print shape of outputs\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/exploratory_analysis.ipynb#ch0000008?line=43'>44</a>\u001b[0m outputs_copy \u001b[39m=\u001b[39m outputs \n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py:1616\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1612'>1613</a>\u001b[0m         decoder_attention_mask \u001b[39m=\u001b[39m decoder_attention_mask\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder\u001b[39m.\u001b[39mfirst_device)\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1614'>1615</a>\u001b[0m \u001b[39m# Decode\u001b[39;00m\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1615'>1616</a>\u001b[0m decoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1616'>1617</a>\u001b[0m     input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1617'>1618</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1618'>1619</a>\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1619'>1620</a>\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1620'>1621</a>\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1621'>1622</a>\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1622'>1623</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mdecoder_head_mask,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1623'>1624</a>\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39;49mcross_attn_head_mask,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1624'>1625</a>\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1625'>1626</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1626'>1627</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1627'>1628</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1628'>1629</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1630'>1631</a>\u001b[0m sequence_output \u001b[39m=\u001b[39m decoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1632'>1633</a>\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py:1011\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=997'>998</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m checkpoint(\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=998'>999</a>\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=999'>1000</a>\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1007'>1008</a>\u001b[0m         \u001b[39mNone\u001b[39;00m,  \u001b[39m# past_key_value is always None with gradient checkpointing\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1008'>1009</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1009'>1010</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1010'>1011</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1011'>1012</a>\u001b[0m         hidden_states,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1012'>1013</a>\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1013'>1014</a>\u001b[0m         position_bias\u001b[39m=\u001b[39;49mposition_bias,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1014'>1015</a>\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1015'>1016</a>\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1016'>1017</a>\u001b[0m         encoder_decoder_position_bias\u001b[39m=\u001b[39;49mencoder_decoder_position_bias,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1017'>1018</a>\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1018'>1019</a>\u001b[0m         cross_attn_layer_head_mask\u001b[39m=\u001b[39;49mcross_attn_layer_head_mask,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1019'>1020</a>\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1020'>1021</a>\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1021'>1022</a>\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1022'>1023</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1024'>1025</a>\u001b[0m \u001b[39m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1025'>1026</a>\u001b[0m \u001b[39m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1026'>1027</a>\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py:698\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=694'>695</a>\u001b[0m     attention_outputs \u001b[39m=\u001b[39m attention_outputs \u001b[39m+\u001b[39m cross_attention_outputs[\u001b[39m2\u001b[39m:]\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=696'>697</a>\u001b[0m \u001b[39m# Apply Feed Forward layer\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=697'>698</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m](hidden_states)\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=699'>700</a>\u001b[0m \u001b[39m# clamp inf values to enable fp16 training\u001b[39;00m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=700'>701</a>\u001b[0m \u001b[39mif\u001b[39;00m hidden_states\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m torch\u001b[39m.\u001b[39mfloat16 \u001b[39mand\u001b[39;00m torch\u001b[39m.\u001b[39misinf(hidden_states)\u001b[39m.\u001b[39many():\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py:308\u001b[0m, in \u001b[0;36mT5LayerFF.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=305'>306</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states):\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=306'>307</a>\u001b[0m     forwarded_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=307'>308</a>\u001b[0m     forwarded_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mDenseReluDense(forwarded_states)\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=308'>309</a>\u001b[0m     hidden_states \u001b[39m=\u001b[39m hidden_states \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(forwarded_states)\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=309'>310</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py:269\u001b[0m, in \u001b[0;36mT5DenseReluDense.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=266'>267</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mrelu(hidden_states)\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=267'>268</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=268'>269</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwo(hidden_states)\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=269'>270</a>\u001b[0m \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/linear.py?line=101'>102</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/linear.py?line=102'>103</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/functional.py:1848\u001b[0m, in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/functional.py?line=1845'>1846</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight, bias):\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/functional.py?line=1846'>1847</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[39minput\u001b[39m, weight, bias), \u001b[39minput\u001b[39m, weight, bias\u001b[39m=\u001b[39mbias)\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/functional.py?line=1847'>1848</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, weight, bias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import T5ForConditionalGeneration\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\") \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\", use_fast=True)\n",
    "\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.train()\n",
    "for batch, (\n",
    "    token_type_ids, \n",
    "    attention_mask, \n",
    "    input_ids, \n",
    "    target_ids\n",
    ") in enumerate(train_loader):\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    token_type_ids = token_type_ids.to(device).long()\n",
    "    attention_mask = attention_mask.to(device).long()\n",
    "    input_ids = input_ids.to(device).long()\n",
    "    target_ids = target_ids.to(device).long()\n",
    "    \n",
    "    # Set the target ids and labels \n",
    "    y_ids = target_ids[:, :-1].contiguous()\n",
    "    lm_labels = target_ids[:, 1:].clone().detach()\n",
    "    \n",
    "    # Make sure that lm_labels are not set for 0 (pad) tokens\n",
    "    lm_labels[target_ids[:, 1:] == tokenizer.pad_token_id] = -100 \n",
    "    \n",
    "    \n",
    "    outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        decoder_input_ids=y_ids,\n",
    "        labels=lm_labels,\n",
    "    )\n",
    "    # print shape of outputs\n",
    "    outputs_copy = outputs \n",
    "    loss = outputs[0]\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    3,     2,     7,  3155,    71,   568, 13205,    16,     3,     9,\n",
      "         3270,    28,  3652,    11,     3,     9,     3, 18458,    36,    15,\n",
      "         5223,    12,    34,     6,    19,  9759,     3,     9,  1871, 13593,\n",
      "           49,   323,     8,  2815,     5,     1,    71,   568,  1067,  9759,\n",
      "            3,     9, 13593,    49,     5,     1,     1,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "['<pad><extra_id_0> A person in a dress with flowers and a stuffed bee attached to it, is pushing a stroller down the street.</s>']\n",
      "<pad><extra_id_0> A person in a dress with flowers and a stuffed bee attached to it, is pushing a stroller down the street.</s>\n"
     ]
    }
   ],
   "source": [
    "# Load t5_model.pt form local directory\n",
    "from torch.optim import AdamW\n",
    "from transformers import T5ForConditionalGeneration\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "# import pad_sequence\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "model.load_state_dict(torch.load(\"t5_model.pt\"))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\", use_fast=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "input_ids_list = []\n",
    "attention_mask_list = []\n",
    "\n",
    "temp_encoding = tokenizer.encode_plus('<s> A person dressed in a dress with flowers and a stuffed bee attached to it, is pushing a baby stroller down the street. </s> A person outside pushing a stroller.</s> ', \n",
    "            max_length=128,\n",
    "            pad_to_max_length=True,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            \n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "\n",
    "input_ids = torch.Tensor(temp_encoding.input_ids)\n",
    "attention_mask = torch.Tensor(temp_encoding.attention_mask)\n",
    "\n",
    "input_ids = input_ids.to(device).long()\n",
    "attention_mask = attention_mask.to(device).long()\n",
    "\n",
    "# input_ids_list.append(input_ids)\n",
    "# attention_mask_list.append(attention_mask)\n",
    "# \n",
    "# input_ids_list = pad_sequence(input_ids_list, batch_first=True)\n",
    "# attention_mask_list = pad_sequence(attention_mask_list, batch_first=True)\n",
    "# print(input_ids_list)\n",
    "# print(type(input_ids_list[0]))\n",
    "print(input_ids)\n",
    "\n",
    "# Set input_ids to be of shape (batch_size, seq_len)\n",
    "input_ids = input_ids.unsqueeze(0) \n",
    "attention_mask = attention_mask.unsqueeze(0)\n",
    "\n",
    "# create new list of inputs and attention_mask\n",
    "input_ids_list = []\n",
    "attention_mask_list = [] \n",
    "\n",
    "model.eval()\n",
    "output = model.generate(\n",
    "    input_ids = input_ids,\n",
    "    attention_mask = attention_mask,\n",
    "    num_beams = 7,\n",
    "    max_length = 100,\n",
    "    repetition_penalty = 2.0,\n",
    "    length_penalty = 0,\n",
    "    early_stopping = False,\n",
    "    use_cache = True,\n",
    "    )\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print([tokenizer.decode(g, skip_special_tokens=False, clean_up_tokenization_spaces=True) for g in output])\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=False, clean_up_tokenization_spaces=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generated_ids = model.generate(\n",
    "                input_ids = input_ids,\n",
    "                attention_mask = attention_mask,\n",
    "                num_beams = 10,\n",
    "                max_length = 100,\n",
    "                repetition_penalty = 1.0,\n",
    "                length_penalty = 1.0,\n",
    "                early_stopping = True,\n",
    "                use_cache = True,\n",
    "                )\n",
    "\n",
    "preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset esnli (/Users/julianbruinsma/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc)\n",
      "100%|██████████| 3/3 [00:00<00:00, 40.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of data: 569033\n",
      " \n",
      "initializing train data loader\n",
      "initializing val data loader\n",
      "initializing test data loader\n",
      "Input ids: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "Attention mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/julianbruinsma/stack/School/Master/Year_2/Natural Language Processing/Final Project/NLP-Final_Project/exploratory_analysis.ipynb Cell 11'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/exploratory_analysis.ipynb#ch0000010?line=20'>21</a>\u001b[0m \u001b[39m# load test loader \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/exploratory_analysis.ipynb#ch0000010?line=21'>22</a>\u001b[0m train_loader, val_loader, test_loader \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mget_data_loaders()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/exploratory_analysis.ipynb#ch0000010?line=23'>24</a>\u001b[0m predictions, ground_truths \u001b[39m=\u001b[39m t5_trainer\u001b[39m.\u001b[39;49mevaluate(model, test_loader, tokenizer, device)\n",
      "File \u001b[0;32m~/stack/School/Master/Year_2/Natural Language Processing/Final Project/NLP-Final_Project/t5_trainer.py:179\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, test_loader, tokenizer, device)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/t5_trainer.py?line=176'>177</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAttention mask:\u001b[39m\u001b[39m\"\u001b[39m, attention_mask)\n\u001b[1;32m    <a href='file:///Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/t5_trainer.py?line=177'>178</a>\u001b[0m \u001b[39m# generate the predictions\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/t5_trainer.py?line=178'>179</a>\u001b[0m generated_ids \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m    <a href='file:///Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/t5_trainer.py?line=179'>180</a>\u001b[0m     input_ids \u001b[39m=\u001b[39;49m input_ids,\n\u001b[1;32m    <a href='file:///Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/t5_trainer.py?line=180'>181</a>\u001b[0m     attention_mask \u001b[39m=\u001b[39;49m attention_mask,\n\u001b[1;32m    <a href='file:///Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/t5_trainer.py?line=181'>182</a>\u001b[0m     num_beams \u001b[39m=\u001b[39;49m \u001b[39m10\u001b[39;49m,\n\u001b[1;32m    <a href='file:///Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/t5_trainer.py?line=182'>183</a>\u001b[0m     max_length \u001b[39m=\u001b[39;49m \u001b[39m100\u001b[39;49m,\n\u001b[1;32m    <a href='file:///Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/t5_trainer.py?line=183'>184</a>\u001b[0m     repetition_penalty \u001b[39m=\u001b[39;49m \u001b[39m1.0\u001b[39;49m,\n\u001b[1;32m    <a href='file:///Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/t5_trainer.py?line=184'>185</a>\u001b[0m     length_penalty \u001b[39m=\u001b[39;49m \u001b[39m1.0\u001b[39;49m,\n\u001b[1;32m    <a href='file:///Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/t5_trainer.py?line=185'>186</a>\u001b[0m     early_stopping \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    <a href='file:///Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/t5_trainer.py?line=186'>187</a>\u001b[0m     use_cache \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    <a href='file:///Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/t5_trainer.py?line=187'>188</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/t5_trainer.py?line=189'>190</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mtruth:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/t5_trainer.py?line=190'>191</a>\u001b[0m \u001b[39mprint\u001b[39m(tokenizer\u001b[39m.\u001b[39mdecode(target_ids[\u001b[39m0\u001b[39m], skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/autograd/grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/autograd/grad_mode.py?line=24'>25</a>\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/autograd/grad_mode.py?line=25'>26</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/autograd/grad_mode.py?line=26'>27</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m():\n\u001b[0;32m---> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/autograd/grad_mode.py?line=27'>28</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/generation_utils.py:1234\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, stopping_criteria, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/generation_utils.py?line=1229'>1230</a>\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/generation_utils.py?line=1230'>1231</a>\u001b[0m         input_ids, expand_size\u001b[39m=\u001b[39mnum_beams, is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/generation_utils.py?line=1231'>1232</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/generation_utils.py?line=1232'>1233</a>\u001b[0m     \u001b[39m# 12. run beam search\u001b[39;00m\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/generation_utils.py?line=1233'>1234</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbeam_search(\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/generation_utils.py?line=1234'>1235</a>\u001b[0m         input_ids,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/generation_utils.py?line=1235'>1236</a>\u001b[0m         beam_scorer,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/generation_utils.py?line=1236'>1237</a>\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/generation_utils.py?line=1237'>1238</a>\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/generation_utils.py?line=1238'>1239</a>\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mpad_token_id,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/generation_utils.py?line=1239'>1240</a>\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49meos_token_id,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/generation_utils.py?line=1240'>1241</a>\u001b[0m         output_scores\u001b[39m=\u001b[39;49moutput_scores,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/generation_utils.py?line=1241'>1242</a>\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/generation_utils.py?line=1242'>1243</a>\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/generation_utils.py?line=1243'>1244</a>\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/generation_utils.py?line=1244'>1245</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/generation_utils.py?line=1246'>1247</a>\u001b[0m \u001b[39melif\u001b[39;00m is_beam_sample_gen_mode:\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/generation_utils.py?line=1247'>1248</a>\u001b[0m     \u001b[39m# 10. prepare logits warper\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/generation_utils.py?line=1248'>1249</a>\u001b[0m     logits_warper \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_logits_warper(\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/generation_utils.py?line=1249'>1250</a>\u001b[0m         top_k\u001b[39m=\u001b[39mtop_k, top_p\u001b[39m=\u001b[39mtop_p, temperature\u001b[39m=\u001b[39mtemperature, num_beams\u001b[39m=\u001b[39mnum_beams\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/generation_utils.py?line=1250'>1251</a>\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/generation_utils.py:2045\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/generation_utils.py?line=2040'>2041</a>\u001b[0m model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/generation_utils.py?line=2041'>2042</a>\u001b[0m     outputs, model_kwargs, is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/generation_utils.py?line=2042'>2043</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/generation_utils.py?line=2043'>2044</a>\u001b[0m \u001b[39mif\u001b[39;00m model_kwargs[\u001b[39m\"\u001b[39m\u001b[39mpast\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/generation_utils.py?line=2044'>2045</a>\u001b[0m     model_kwargs[\u001b[39m\"\u001b[39m\u001b[39mpast\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reorder_cache(model_kwargs[\u001b[39m\"\u001b[39;49m\u001b[39mpast\u001b[39;49m\u001b[39m\"\u001b[39;49m], beam_idx)\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/generation_utils.py?line=2046'>2047</a>\u001b[0m \u001b[39mif\u001b[39;00m return_dict_in_generate \u001b[39mand\u001b[39;00m output_scores:\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/generation_utils.py?line=2047'>2048</a>\u001b[0m     beam_indices \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m((beam_indices[beam_idx[i]] \u001b[39m+\u001b[39m (beam_idx[i],) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(beam_indices))))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py:1714\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration._reorder_cache\u001b[0;34m(self, past, beam_idx)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1709'>1710</a>\u001b[0m reordered_layer_past_states \u001b[39m=\u001b[39m ()\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1710'>1711</a>\u001b[0m \u001b[39mfor\u001b[39;00m layer_past_state \u001b[39min\u001b[39;00m layer_past_states:\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1711'>1712</a>\u001b[0m     \u001b[39m# need to set correct `past` for each of the four key / value states\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1712'>1713</a>\u001b[0m     reordered_layer_past_states \u001b[39m=\u001b[39m reordered_layer_past_states \u001b[39m+\u001b[39m (\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1713'>1714</a>\u001b[0m         layer_past_state\u001b[39m.\u001b[39;49mindex_select(\u001b[39m0\u001b[39;49m, beam_idx\u001b[39m.\u001b[39;49mto(layer_past_state\u001b[39m.\u001b[39;49mdevice)),\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1714'>1715</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1716'>1717</a>\u001b[0m \u001b[39massert\u001b[39;00m reordered_layer_past_states[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m layer_past_states[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1717'>1718</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(reordered_layer_past_states) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(layer_past_states)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load t5_model.pt form local directory\n",
    "from torch.optim import AdamW\n",
    "from transformers import T5ForConditionalGeneration\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "# import pad_sequence\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from dataloader import esnli\n",
    "\n",
    "import t5_trainer\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "model.load_state_dict(torch.load(\"t5_model.pt\"))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\", use_fast=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "dataset = esnli()\n",
    "\n",
    "# load test loader \n",
    "train_loader, val_loader, test_loader = dataset.get_data_loaders()\n",
    "\n",
    "predictions, ground_truths = t5_trainer.evaluate(model, test_loader, tokenizer, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', '', '', '', '', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "predictions, ground_truths = t5_trainer.evaluate(model, test_loader, tokenizer, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0adcc2737ebf6a4a119f135174df96668767fca1ef1112612db5ecadf2b6d608"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
