{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset esnli (/Users/julianbruinsma/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc)\n",
      "100%|██████████| 3/3 [00:00<00:00, 20.38it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"esnli\")\n",
    "\n",
    "# Take subset of data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Get features from the dataset\n",
    "train = pd.DataFrame.from_dict(dataset['train'])\n",
    "\n",
    "# Take first 10 examples\n",
    "df = train.iloc[:10]\n",
    "\n",
    "\n",
    "print(type(df['premise']))\n",
    "\n",
    "# from train dataframe, get the premise and hypothesis\n",
    "premise = df['premise']\n",
    "hypothesis = df['hypothesis']\n",
    "\n",
    "# print(premise)\n",
    "# print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral </s> the person is not necessarily training his horse </s>\n",
      "contradiction </s> One cannot be on a jumping horse cannot be a diner ordering food. </s>\n",
      "entailment </s> a broken down airplane is outdoors </s>\n",
      "neutral </s> Just because they are smiling and waving at a camera does not imply their parents or anyone is anyone behind it </s>\n",
      "entailment </s> The children must be present to see them smiling and waving. </s>\n",
      "contradiction </s> One cannot be smiling and frowning at the same time. </s>\n",
      "contradiction </s> One cannot be in the middle of a bridge if they are on the sidewalk. </s>\n",
      "entailment </s> jumping on skateboard is the same as doing trick on skateboard. </s>\n",
      "neutral </s> Just because the boy is jumping on a skateboard does not imply he is wearing safety equipment </s>\n",
      "neutral </s> it is not necessarily true the man drinks his juice </s>\n",
      "{'input_ids': [7163, 1, 34, 19, 59, 6539, 1176, 8, 388, 6750, 112, 5143, 1, 1], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [389, 2749, 388, 2561, 7, 28, 112, 5470, 5143, 44, 3, 9, 422, 953, 16, 3, 9, 1975, 1814, 298, 1652, 16, 2756, 11999, 3, 9955, 3993, 16, 8, 2458, 5, 1, 389, 2749, 388, 6750, 112, 5143, 38, 3, 88, 1749, 7, 21, 112, 3062, 12, 129, 326, 161, 5, 1], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "# import pad_sequence\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\", use_fast=True)\n",
    "\n",
    "# concatenate premise and hypothesis with separator token </s> \n",
    "input_ids = []\n",
    "attention_mask = []\n",
    "token_type_ids = []\n",
    "target_ids = []\n",
    "\n",
    "premise_list = df[\"premise\"].tolist()\n",
    "hypothesis_list = df[\"hypothesis\"].tolist()\n",
    "explanation_list = df[\"explanation_1\"].tolist()\n",
    "label_list = df[\"label\"].tolist()\n",
    "\n",
    "for (premise, hypothesis, explanation, label) in zip(premise_list, hypothesis_list, explanation_list, label_list):\n",
    "    premise_hypothesis = f\"{premise} </s> {hypothesis}\"\n",
    "    \n",
    "    # make switch case for label if it is 0 label = entailment, if it is 1 label = neutral, if it is 2 label = contradiction\n",
    "    if label == 0:\n",
    "        label = \"entailment\"\n",
    "    elif label == 1:\n",
    "        label = \"neutral\"\n",
    "    elif label == 2:\n",
    "        label = \"contradiction\"\n",
    "    \n",
    "    # Concatenate label with explanation\n",
    "\n",
    "    label_explanation = f\"{label} </s> {explanation} </s>\" \n",
    "    print(label_explanation)\n",
    "    \n",
    "    \n",
    "    hypothesis_premise_tokens = tokenizer.encode_plus(\n",
    "        premise_hypothesis,\n",
    "        truncation=True, \n",
    "        return_token_type_ids=True, \n",
    "        max_length=128,\n",
    "        )\n",
    "    \n",
    "    target_encoding = tokenizer.encode_plus(\n",
    "        label_explanation,\n",
    "        truncation=True,\n",
    "        padding=\"longest\",\n",
    "        return_token_type_ids=True,\n",
    "        max_length=128,\n",
    "    )\n",
    "    \n",
    "\n",
    "    token_type_ids.append(torch.Tensor(hypothesis_premise_tokens.token_type_ids))\n",
    "    attention_mask.append(torch.Tensor(hypothesis_premise_tokens.attention_mask))\n",
    "    input_ids.append(torch.Tensor(hypothesis_premise_tokens.input_ids))\n",
    "    target_ids.append(torch.Tensor(target_encoding.input_ids))\n",
    "\n",
    "token_type_ids = pad_sequence(token_type_ids, batch_first=True)\n",
    "attention_mask = pad_sequence(attention_mask, batch_first=True)\n",
    "input_ids = pad_sequence(input_ids, batch_first=True)\n",
    "target = pad_sequence(target_ids, batch_first=True)\n",
    "\n",
    "dataset = TensorDataset(\n",
    "    input_ids,\n",
    "    attention_mask,\n",
    "    token_type_ids,\n",
    "    target\n",
    ")\n",
    "\n",
    "print(target_encoding)\n",
    "print(hypothesis_premise_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  568,    19,    59,  6539,   761,   112,  4952,     1,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100],\n",
      "        [ 1178,    36,    30,     3,     9, 15539,  4952,  1178,    36,     3,\n",
      "             9,   176,    49, 12320,   542,     5,     1,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100],\n",
      "        [    9,  4335,   323, 20527,    19, 10962,     1,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100],\n",
      "        [  250,    79,    33, 20770,    11,  8036,  3745,    44,     3,     9,\n",
      "          1861,   405,    59,     3, 18531,    70,  1362,    42,  1321,    19,\n",
      "          1321,  1187,    34,     1],\n",
      "        [  502,   398,    36,   915,    12,   217,   135, 20770,    11,  8036,\n",
      "          3745,     5,     1,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100],\n",
      "        [ 1178,    36, 20770,    11,  2515,  9197,    53,    44,     8,   337,\n",
      "            97,     5,     1,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100],\n",
      "        [ 1178,    36,    16,     8,  2214,    13,     3,     9,  4716,     3,\n",
      "            99,    79,    33,    30,     8, 21393,     5,     1,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100],\n",
      "        [   30, 16573,  1976,    19,     8,   337,    38,   692,  7873,    30,\n",
      "         16573,  1976,     5,     1,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100],\n",
      "        [  250,     8,  4940,    19, 15539,    30,     3,     9, 16573,  1976,\n",
      "           405,    59,     3, 18531,     3,    88,    19,  5119,  1455,  1277,\n",
      "             1,  -100,  -100,  -100],\n",
      "        [   19,    59,  6539,  1176,     8,   388,  6750,   112,  5143,     1,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100]])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "target_device = target.to(device).long()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# get shape of target device\n",
    "# print(target_device[:, :-1].contiguous())\n",
    "# \n",
    "# \n",
    "# print(\"detaching\")\n",
    "# print(target_device[:, 1:].clone().detach())\n",
    "# print(target_device[0].clone().detach())\n",
    "\n",
    "lm_labels = target_device[:, 1:].clone().detach()\n",
    "lm_labels[target_device[:, 1:] == tokenizer.pad_token_id] = -100 \n",
    "print(lm_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset esnli (/Users/julianbruinsma/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc)\n",
      "100%|██████████| 3/3 [00:00<00:00, 75.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing data loader\n",
      "initializing train data loader\n",
      "initializing val data loader\n",
      "initializing test data loader\n",
      "<class 'torch.utils.data.dataloader.DataLoader'>\n"
     ]
    }
   ],
   "source": [
    "from dataloader import esnli\n",
    "\n",
    "print(\"initializing dataset\")\n",
    "dataset = esnli()\n",
    "print(\"initializing data loader\")\n",
    "train_loader, val_loader, test_loader = dataset.get_data_loaders()\n",
    "\n",
    "print(type(train_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog - he is a joy to walk with and is very affectionate with me. I like to spend time with him on walks with his kitty cat, leo, who is so cute!\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "input_ids = tokenizer.encode('summarize: I enjoy walking with my cute dog', return_tensors='pt')\n",
    "greedy_output = model.generate(input_ids, num_beams=7, no_repeat_ngram_size=2, min_length=50, max_length=100)\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 555, 1178,   36,  ...,    0,    0,    0],\n",
      "        [ 466,    8, 1021,  ...,    0,    0,    0],\n",
      "        [   8,  861,   19,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [  37, 4940,   19,  ...,    0,    0,    0],\n",
      "        [ 290,   19,  893,  ...,    0,    0,    0],\n",
      "        [  37, 1076,   33,  ...,    0,    0,    0]])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/julianbruinsma/stack/School/Master/Year_2/Natural Language Processing/Final Project/NLP-Final_Project/exploratory_analysis.ipynb Cell 8'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/exploratory_analysis.ipynb#ch0000008?line=32'>33</a>\u001b[0m \u001b[39m# Make sure that lm_labels are not set for 0 (pad) tokens\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/exploratory_analysis.ipynb#ch0000008?line=33'>34</a>\u001b[0m lm_labels[target_ids[:, \u001b[39m1\u001b[39m:] \u001b[39m==\u001b[39m tokenizer\u001b[39m.\u001b[39mpad_token_id] \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m100\u001b[39m \n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/exploratory_analysis.ipynb#ch0000008?line=36'>37</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/exploratory_analysis.ipynb#ch0000008?line=37'>38</a>\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/exploratory_analysis.ipynb#ch0000008?line=38'>39</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/exploratory_analysis.ipynb#ch0000008?line=39'>40</a>\u001b[0m     decoder_input_ids\u001b[39m=\u001b[39;49my_ids,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/exploratory_analysis.ipynb#ch0000008?line=40'>41</a>\u001b[0m     labels\u001b[39m=\u001b[39;49mlm_labels,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/exploratory_analysis.ipynb#ch0000008?line=41'>42</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/exploratory_analysis.ipynb#ch0000008?line=42'>43</a>\u001b[0m \u001b[39m# print shape of outputs\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/exploratory_analysis.ipynb#ch0000008?line=43'>44</a>\u001b[0m outputs_copy \u001b[39m=\u001b[39m outputs \n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py:1616\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1612'>1613</a>\u001b[0m         decoder_attention_mask \u001b[39m=\u001b[39m decoder_attention_mask\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder\u001b[39m.\u001b[39mfirst_device)\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1614'>1615</a>\u001b[0m \u001b[39m# Decode\u001b[39;00m\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1615'>1616</a>\u001b[0m decoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1616'>1617</a>\u001b[0m     input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1617'>1618</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1618'>1619</a>\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1619'>1620</a>\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1620'>1621</a>\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1621'>1622</a>\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1622'>1623</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mdecoder_head_mask,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1623'>1624</a>\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39;49mcross_attn_head_mask,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1624'>1625</a>\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1625'>1626</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1626'>1627</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1627'>1628</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1628'>1629</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1630'>1631</a>\u001b[0m sequence_output \u001b[39m=\u001b[39m decoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1632'>1633</a>\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py:1011\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=997'>998</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m checkpoint(\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=998'>999</a>\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=999'>1000</a>\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1007'>1008</a>\u001b[0m         \u001b[39mNone\u001b[39;00m,  \u001b[39m# past_key_value is always None with gradient checkpointing\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1008'>1009</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1009'>1010</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1010'>1011</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1011'>1012</a>\u001b[0m         hidden_states,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1012'>1013</a>\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1013'>1014</a>\u001b[0m         position_bias\u001b[39m=\u001b[39;49mposition_bias,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1014'>1015</a>\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1015'>1016</a>\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1016'>1017</a>\u001b[0m         encoder_decoder_position_bias\u001b[39m=\u001b[39;49mencoder_decoder_position_bias,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1017'>1018</a>\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1018'>1019</a>\u001b[0m         cross_attn_layer_head_mask\u001b[39m=\u001b[39;49mcross_attn_layer_head_mask,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1019'>1020</a>\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1020'>1021</a>\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1021'>1022</a>\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1022'>1023</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1024'>1025</a>\u001b[0m \u001b[39m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1025'>1026</a>\u001b[0m \u001b[39m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1026'>1027</a>\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py:698\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=694'>695</a>\u001b[0m     attention_outputs \u001b[39m=\u001b[39m attention_outputs \u001b[39m+\u001b[39m cross_attention_outputs[\u001b[39m2\u001b[39m:]\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=696'>697</a>\u001b[0m \u001b[39m# Apply Feed Forward layer\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=697'>698</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m](hidden_states)\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=699'>700</a>\u001b[0m \u001b[39m# clamp inf values to enable fp16 training\u001b[39;00m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=700'>701</a>\u001b[0m \u001b[39mif\u001b[39;00m hidden_states\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m torch\u001b[39m.\u001b[39mfloat16 \u001b[39mand\u001b[39;00m torch\u001b[39m.\u001b[39misinf(hidden_states)\u001b[39m.\u001b[39many():\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py:308\u001b[0m, in \u001b[0;36mT5LayerFF.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=305'>306</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states):\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=306'>307</a>\u001b[0m     forwarded_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=307'>308</a>\u001b[0m     forwarded_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mDenseReluDense(forwarded_states)\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=308'>309</a>\u001b[0m     hidden_states \u001b[39m=\u001b[39m hidden_states \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(forwarded_states)\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=309'>310</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py:269\u001b[0m, in \u001b[0;36mT5DenseReluDense.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=266'>267</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mrelu(hidden_states)\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=267'>268</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=268'>269</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwo(hidden_states)\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=269'>270</a>\u001b[0m \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/linear.py?line=101'>102</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/linear.py?line=102'>103</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/functional.py:1848\u001b[0m, in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/functional.py?line=1845'>1846</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight, bias):\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/functional.py?line=1846'>1847</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[39minput\u001b[39m, weight, bias), \u001b[39minput\u001b[39m, weight, bias\u001b[39m=\u001b[39mbias)\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/functional.py?line=1847'>1848</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, weight, bias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import T5ForConditionalGeneration\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\") \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\", use_fast=True)\n",
    "\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.train()\n",
    "for batch, (\n",
    "    token_type_ids, \n",
    "    attention_mask, \n",
    "    input_ids, \n",
    "    target_ids\n",
    ") in enumerate(train_loader):\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    token_type_ids = token_type_ids.to(device).long()\n",
    "    attention_mask = attention_mask.to(device).long()\n",
    "    input_ids = input_ids.to(device).long()\n",
    "    target_ids = target_ids.to(device).long()\n",
    "    \n",
    "    # Set the target ids and labels \n",
    "    y_ids = target_ids[:, :-1].contiguous()\n",
    "    lm_labels = target_ids[:, 1:].clone().detach()\n",
    "    \n",
    "    # Make sure that lm_labels are not set for 0 (pad) tokens\n",
    "    lm_labels[target_ids[:, 1:] == tokenizer.pad_token_id] = -100 \n",
    "    \n",
    "    \n",
    "    outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        decoder_input_ids=y_ids,\n",
    "        labels=lm_labels,\n",
    "    )\n",
    "    # print shape of outputs\n",
    "    outputs_copy = outputs \n",
    "    loss = outputs[0]\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/julianbruinsma/stack/School/Master/Year_2/Natural Language Processing/Final Project/NLP-Final_Project/exploratory_analysis.ipynb Cell 9'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/exploratory_analysis.ipynb#ch0000009?line=25'>26</a>\u001b[0m target_ids_list \u001b[39m=\u001b[39m pad_sequence(target_ids_list, batch_first\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/exploratory_analysis.ipynb#ch0000009?line=26'>27</a>\u001b[0m attention_mask_list \u001b[39m=\u001b[39m pad_sequence(attention_mask_list, batch_first\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/exploratory_analysis.ipynb#ch0000009?line=28'>29</a>\u001b[0m output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/exploratory_analysis.ipynb#ch0000009?line=29'>30</a>\u001b[0m     input_ids \u001b[39m=\u001b[39;49m input_ids, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/exploratory_analysis.ipynb#ch0000009?line=30'>31</a>\u001b[0m     attention_mask \u001b[39m=\u001b[39;49m attention_mask, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/exploratory_analysis.ipynb#ch0000009?line=31'>32</a>\u001b[0m     num_beams\u001b[39m=\u001b[39;49m\u001b[39m7\u001b[39;49m, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/exploratory_analysis.ipynb#ch0000009?line=32'>33</a>\u001b[0m     no_repeat_ngram_size\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/exploratory_analysis.ipynb#ch0000009?line=33'>34</a>\u001b[0m     min_length\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/exploratory_analysis.ipynb#ch0000009?line=34'>35</a>\u001b[0m     max_length\u001b[39m=\u001b[39;49m\u001b[39m80\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/exploratory_analysis.ipynb#ch0000009?line=35'>36</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/exploratory_analysis.ipynb#ch0000009?line=36'>37</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mOutput:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m100\u001b[39m \u001b[39m*\u001b[39m \u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/exploratory_analysis.ipynb#ch0000009?line=37'>38</a>\u001b[0m \u001b[39mprint\u001b[39m(tokenizer\u001b[39m.\u001b[39mdecode(output[\u001b[39m0\u001b[39m], skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/autograd/grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/autograd/grad_mode.py?line=24'>25</a>\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/autograd/grad_mode.py?line=25'>26</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/autograd/grad_mode.py?line=26'>27</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m():\n\u001b[0;32m---> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/autograd/grad_mode.py?line=27'>28</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/generation_utils.py:1088\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, stopping_criteria, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/generation_utils.py?line=1080'>1081</a>\u001b[0m     model_kwargs[\u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_attention_mask_for_generation(\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/generation_utils.py?line=1081'>1082</a>\u001b[0m         inputs_tensor, pad_token_id, eos_token_id\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/generation_utils.py?line=1082'>1083</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/generation_utils.py?line=1084'>1085</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mencoder_outputs\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m model_kwargs:\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/generation_utils.py?line=1085'>1086</a>\u001b[0m     \u001b[39m# if model is encoder decoder encoder_outputs are created\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/generation_utils.py?line=1086'>1087</a>\u001b[0m     \u001b[39m# and added to `model_kwargs`\u001b[39;00m\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/generation_utils.py?line=1087'>1088</a>\u001b[0m     model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_encoder_decoder_kwargs_for_generation(\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/generation_utils.py?line=1088'>1089</a>\u001b[0m         inputs_tensor, model_kwargs, model_input_name\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/generation_utils.py?line=1089'>1090</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/generation_utils.py?line=1091'>1092</a>\u001b[0m \u001b[39m# 4. Prepare `input_ids` which will be used for auto-regressive generation\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/generation_utils.py?line=1092'>1093</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/generation_utils.py:507\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_encoder_decoder_kwargs_for_generation\u001b[0;34m(self, inputs_tensor, model_kwargs, model_input_name)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/generation_utils.py?line=504'>505</a>\u001b[0m encoder_kwargs[\u001b[39m\"\u001b[39m\u001b[39mreturn_dict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/generation_utils.py?line=505'>506</a>\u001b[0m encoder_kwargs[model_input_name] \u001b[39m=\u001b[39m inputs_tensor\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/generation_utils.py?line=506'>507</a>\u001b[0m model_kwargs[\u001b[39m\"\u001b[39m\u001b[39mencoder_outputs\u001b[39m\u001b[39m\"\u001b[39m]: ModelOutput \u001b[39m=\u001b[39m encoder(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mencoder_kwargs)\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/generation_utils.py?line=508'>509</a>\u001b[0m \u001b[39mreturn\u001b[39;00m model_kwargs\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py:912\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=909'>910</a>\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=910'>911</a>\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_tokens \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mYou have to initialize the model with valid token embeddings\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=911'>912</a>\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_tokens(input_ids)\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=913'>914</a>\u001b[0m batch_size, seq_length \u001b[39m=\u001b[39m input_shape\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=915'>916</a>\u001b[0m \u001b[39m# required mask seq length can be calculated via length of past\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/sparse.py:158\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/sparse.py?line=156'>157</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/sparse.py?line=157'>158</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/sparse.py?line=158'>159</a>\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/sparse.py?line=159'>160</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/functional.py:2044\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/functional.py?line=2037'>2038</a>\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/functional.py?line=2038'>2039</a>\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/functional.py?line=2039'>2040</a>\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/functional.py?line=2040'>2041</a>\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/functional.py?line=2041'>2042</a>\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/functional.py?line=2042'>2043</a>\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/functional.py?line=2043'>2044</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "# Load t5_model.pt form local directory\n",
    "from torch.optim import AdamW\n",
    "from transformers import T5ForConditionalGeneration\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "# import pad_sequence\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "model.load_state_dict(torch.load(\"t5_model.pt\"))\n",
    "\n",
    "target_ids_list = []\n",
    "attention_mask_list = []\n",
    "\n",
    "temp_encoding = tokenizer.encode_plus('This church choir sings to the masses as they sing joyous songs from the book at a church. </s> The church has cracks in the ceiling. </s>', \n",
    "                truncation=True, \n",
    "                return_token_type_ids=True, \n",
    "                max_length=256)\n",
    "\n",
    "input_ids = torch.Tensor(temp_encoding.input_ids)\n",
    "attention_mask = torch.Tensor(temp_encoding.attention_mask)\n",
    "\n",
    "target_ids_list.append(input_ids)\n",
    "attention_mask_list.append(attention_mask)\n",
    "\n",
    "target_ids_list = pad_sequence(target_ids_list, batch_first=True)\n",
    "attention_mask_list = pad_sequence(attention_mask_list, batch_first=True)\n",
    "\n",
    "output = model.generate(\n",
    "    input_ids = input_ids, \n",
    "    attention_mask = attention_mask, \n",
    "    num_beams=7, \n",
    "    no_repeat_ngram_size=2, \n",
    "    min_length=50, \n",
    "    max_length=80\n",
    "    )\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generated_ids = model.generate(\n",
    "                input_ids = input_ids,\n",
    "                attention_mask = attention_mask,\n",
    "                num_beams = 10,\n",
    "                max_length = 100,\n",
    "                repetition_penalty = 1.0,\n",
    "                length_penalty = 1.0,\n",
    "                early_stopping = True,\n",
    "                use_cache = True,\n",
    "                )\n",
    "\n",
    "preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0adcc2737ebf6a4a119f135174df96668767fca1ef1112612db5ecadf2b6d608"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
