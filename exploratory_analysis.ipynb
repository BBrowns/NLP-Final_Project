{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset esnli (/Users/julianbruinsma/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc)\n",
      "100%|██████████| 3/3 [00:00<00:00, 20.38it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"esnli\")\n",
    "\n",
    "# Take subset of data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Get features from the dataset\n",
    "train = pd.DataFrame.from_dict(dataset['train'])\n",
    "\n",
    "# Take first 10 examples\n",
    "df = train.iloc[:10]\n",
    "\n",
    "\n",
    "print(type(df['premise']))\n",
    "\n",
    "# from train dataframe, get the premise and hypothesis\n",
    "premise = df['premise']\n",
    "hypothesis = df['hypothesis']\n",
    "\n",
    "# print(premise)\n",
    "# print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral </s> the person is not necessarily training his horse </s>\n",
      "contradiction </s> One cannot be on a jumping horse cannot be a diner ordering food. </s>\n",
      "entailment </s> a broken down airplane is outdoors </s>\n",
      "neutral </s> Just because they are smiling and waving at a camera does not imply their parents or anyone is anyone behind it </s>\n",
      "entailment </s> The children must be present to see them smiling and waving. </s>\n",
      "contradiction </s> One cannot be smiling and frowning at the same time. </s>\n",
      "contradiction </s> One cannot be in the middle of a bridge if they are on the sidewalk. </s>\n",
      "entailment </s> jumping on skateboard is the same as doing trick on skateboard. </s>\n",
      "neutral </s> Just because the boy is jumping on a skateboard does not imply he is wearing safety equipment </s>\n",
      "neutral </s> it is not necessarily true the man drinks his juice </s>\n",
      "{'input_ids': [7163, 1, 34, 19, 59, 6539, 1176, 8, 388, 6750, 112, 5143, 1, 1], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [389, 2749, 388, 2561, 7, 28, 112, 5470, 5143, 44, 3, 9, 422, 953, 16, 3, 9, 1975, 1814, 298, 1652, 16, 2756, 11999, 3, 9955, 3993, 16, 8, 2458, 5, 1, 389, 2749, 388, 6750, 112, 5143, 38, 3, 88, 1749, 7, 21, 112, 3062, 12, 129, 326, 161, 5, 1], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "# import pad_sequence\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\", use_fast=True)\n",
    "\n",
    "# concatenate premise and hypothesis with separator token </s> \n",
    "input_ids = []\n",
    "attention_mask = []\n",
    "token_type_ids = []\n",
    "target_ids = []\n",
    "\n",
    "premise_list = df[\"premise\"].tolist()\n",
    "hypothesis_list = df[\"hypothesis\"].tolist()\n",
    "explanation_list = df[\"explanation_1\"].tolist()\n",
    "label_list = df[\"label\"].tolist()\n",
    "\n",
    "for (premise, hypothesis, explanation, label) in zip(premise_list, hypothesis_list, explanation_list, label_list):\n",
    "    premise_hypothesis = f\"{premise} </s> {hypothesis}\"\n",
    "    \n",
    "    # make switch case for label if it is 0 label = entailment, if it is 1 label = neutral, if it is 2 label = contradiction\n",
    "    if label == 0:\n",
    "        label = \"entailment\"\n",
    "    elif label == 1:\n",
    "        label = \"neutral\"\n",
    "    elif label == 2:\n",
    "        label = \"contradiction\"\n",
    "    \n",
    "    # Concatenate label with explanation\n",
    "\n",
    "    label_explanation = f\"{label} </s> {explanation} </s>\" \n",
    "    print(label_explanation)\n",
    "    \n",
    "    \n",
    "    hypothesis_premise_tokens = tokenizer.encode_plus(\n",
    "        premise_hypothesis,\n",
    "        truncation=True, \n",
    "        return_token_type_ids=True, \n",
    "        max_length=128,\n",
    "        )\n",
    "    \n",
    "    target_encoding = tokenizer.encode_plus(\n",
    "        label_explanation,\n",
    "        truncation=True,\n",
    "        padding=\"longest\",\n",
    "        return_token_type_ids=True,\n",
    "        max_length=128,\n",
    "    )\n",
    "    \n",
    "\n",
    "    token_type_ids.append(torch.Tensor(hypothesis_premise_tokens.token_type_ids))\n",
    "    attention_mask.append(torch.Tensor(hypothesis_premise_tokens.attention_mask))\n",
    "    input_ids.append(torch.Tensor(hypothesis_premise_tokens.input_ids))\n",
    "    target_ids.append(torch.Tensor(target_encoding.input_ids))\n",
    "\n",
    "token_type_ids = pad_sequence(token_type_ids, batch_first=True)\n",
    "attention_mask = pad_sequence(attention_mask, batch_first=True)\n",
    "input_ids = pad_sequence(input_ids, batch_first=True)\n",
    "target = pad_sequence(target_ids, batch_first=True)\n",
    "\n",
    "dataset = TensorDataset(\n",
    "    input_ids,\n",
    "    attention_mask,\n",
    "    token_type_ids,\n",
    "    target\n",
    ")\n",
    "\n",
    "print(target_encoding)\n",
    "print(hypothesis_premise_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  568,    19,    59,  6539,   761,   112,  4952,     1,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100],\n",
      "        [ 1178,    36,    30,     3,     9, 15539,  4952,  1178,    36,     3,\n",
      "             9,   176,    49, 12320,   542,     5,     1,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100],\n",
      "        [    9,  4335,   323, 20527,    19, 10962,     1,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100],\n",
      "        [  250,    79,    33, 20770,    11,  8036,  3745,    44,     3,     9,\n",
      "          1861,   405,    59,     3, 18531,    70,  1362,    42,  1321,    19,\n",
      "          1321,  1187,    34,     1],\n",
      "        [  502,   398,    36,   915,    12,   217,   135, 20770,    11,  8036,\n",
      "          3745,     5,     1,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100],\n",
      "        [ 1178,    36, 20770,    11,  2515,  9197,    53,    44,     8,   337,\n",
      "            97,     5,     1,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100],\n",
      "        [ 1178,    36,    16,     8,  2214,    13,     3,     9,  4716,     3,\n",
      "            99,    79,    33,    30,     8, 21393,     5,     1,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100],\n",
      "        [   30, 16573,  1976,    19,     8,   337,    38,   692,  7873,    30,\n",
      "         16573,  1976,     5,     1,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100],\n",
      "        [  250,     8,  4940,    19, 15539,    30,     3,     9, 16573,  1976,\n",
      "           405,    59,     3, 18531,     3,    88,    19,  5119,  1455,  1277,\n",
      "             1,  -100,  -100,  -100],\n",
      "        [   19,    59,  6539,  1176,     8,   388,  6750,   112,  5143,     1,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100]])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "target_device = target.to(device).long()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# get shape of target device\n",
    "# print(target_device[:, :-1].contiguous())\n",
    "# \n",
    "# \n",
    "# print(\"detaching\")\n",
    "# print(target_device[:, 1:].clone().detach())\n",
    "# print(target_device[0].clone().detach())\n",
    "\n",
    "lm_labels = target_device[:, 1:].clone().detach()\n",
    "lm_labels[target_device[:, 1:] == tokenizer.pad_token_id] = -100 \n",
    "print(lm_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset esnli (/Users/julianbruinsma/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc)\n",
      "100%|██████████| 3/3 [00:00<00:00, 75.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing data loader\n",
      "initializing train data loader\n",
      "initializing val data loader\n",
      "initializing test data loader\n",
      "<class 'torch.utils.data.dataloader.DataLoader'>\n"
     ]
    }
   ],
   "source": [
    "from dataloader import esnli\n",
    "\n",
    "print(\"initializing dataset\")\n",
    "dataset = esnli()\n",
    "print(\"initializing data loader\")\n",
    "train_loader, val_loader, test_loader = dataset.get_data_loaders()\n",
    "\n",
    "print(type(train_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog - he is a joy to walk with and is very affectionate with me. I like to spend time with him on walks with his kitty cat, leo, who is so cute!\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "input_ids = tokenizer.encode('summarize: I enjoy walking with my cute dog', return_tensors='pt')\n",
    "greedy_output = model.generate(input_ids, num_beams=7, no_repeat_ngram_size=2, min_length=50, max_length=100)\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 555, 1178,   36,  ...,    0,    0,    0],\n",
      "        [ 466,    8, 1021,  ...,    0,    0,    0],\n",
      "        [   8,  861,   19,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [  37, 4940,   19,  ...,    0,    0,    0],\n",
      "        [ 290,   19,  893,  ...,    0,    0,    0],\n",
      "        [  37, 1076,   33,  ...,    0,    0,    0]])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/julianbruinsma/stack/School/Master/Year_2/Natural Language Processing/Final Project/NLP-Final_Project/exploratory_analysis.ipynb Cell 8'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/exploratory_analysis.ipynb#ch0000008?line=32'>33</a>\u001b[0m \u001b[39m# Make sure that lm_labels are not set for 0 (pad) tokens\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/exploratory_analysis.ipynb#ch0000008?line=33'>34</a>\u001b[0m lm_labels[target_ids[:, \u001b[39m1\u001b[39m:] \u001b[39m==\u001b[39m tokenizer\u001b[39m.\u001b[39mpad_token_id] \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m100\u001b[39m \n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/exploratory_analysis.ipynb#ch0000008?line=36'>37</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/exploratory_analysis.ipynb#ch0000008?line=37'>38</a>\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/exploratory_analysis.ipynb#ch0000008?line=38'>39</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/exploratory_analysis.ipynb#ch0000008?line=39'>40</a>\u001b[0m     decoder_input_ids\u001b[39m=\u001b[39;49my_ids,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/exploratory_analysis.ipynb#ch0000008?line=40'>41</a>\u001b[0m     labels\u001b[39m=\u001b[39;49mlm_labels,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/exploratory_analysis.ipynb#ch0000008?line=41'>42</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/exploratory_analysis.ipynb#ch0000008?line=42'>43</a>\u001b[0m \u001b[39m# print shape of outputs\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/exploratory_analysis.ipynb#ch0000008?line=43'>44</a>\u001b[0m outputs_copy \u001b[39m=\u001b[39m outputs \n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py:1616\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1612'>1613</a>\u001b[0m         decoder_attention_mask \u001b[39m=\u001b[39m decoder_attention_mask\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder\u001b[39m.\u001b[39mfirst_device)\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1614'>1615</a>\u001b[0m \u001b[39m# Decode\u001b[39;00m\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1615'>1616</a>\u001b[0m decoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1616'>1617</a>\u001b[0m     input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1617'>1618</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1618'>1619</a>\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1619'>1620</a>\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1620'>1621</a>\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1621'>1622</a>\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1622'>1623</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mdecoder_head_mask,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1623'>1624</a>\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39;49mcross_attn_head_mask,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1624'>1625</a>\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1625'>1626</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1626'>1627</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1627'>1628</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1628'>1629</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1630'>1631</a>\u001b[0m sequence_output \u001b[39m=\u001b[39m decoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1632'>1633</a>\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py:1011\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=997'>998</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m checkpoint(\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=998'>999</a>\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=999'>1000</a>\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1007'>1008</a>\u001b[0m         \u001b[39mNone\u001b[39;00m,  \u001b[39m# past_key_value is always None with gradient checkpointing\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1008'>1009</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1009'>1010</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1010'>1011</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1011'>1012</a>\u001b[0m         hidden_states,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1012'>1013</a>\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1013'>1014</a>\u001b[0m         position_bias\u001b[39m=\u001b[39;49mposition_bias,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1014'>1015</a>\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1015'>1016</a>\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1016'>1017</a>\u001b[0m         encoder_decoder_position_bias\u001b[39m=\u001b[39;49mencoder_decoder_position_bias,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1017'>1018</a>\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1018'>1019</a>\u001b[0m         cross_attn_layer_head_mask\u001b[39m=\u001b[39;49mcross_attn_layer_head_mask,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1019'>1020</a>\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1020'>1021</a>\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1021'>1022</a>\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1022'>1023</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1024'>1025</a>\u001b[0m \u001b[39m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1025'>1026</a>\u001b[0m \u001b[39m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1026'>1027</a>\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py:698\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=694'>695</a>\u001b[0m     attention_outputs \u001b[39m=\u001b[39m attention_outputs \u001b[39m+\u001b[39m cross_attention_outputs[\u001b[39m2\u001b[39m:]\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=696'>697</a>\u001b[0m \u001b[39m# Apply Feed Forward layer\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=697'>698</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m](hidden_states)\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=699'>700</a>\u001b[0m \u001b[39m# clamp inf values to enable fp16 training\u001b[39;00m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=700'>701</a>\u001b[0m \u001b[39mif\u001b[39;00m hidden_states\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m torch\u001b[39m.\u001b[39mfloat16 \u001b[39mand\u001b[39;00m torch\u001b[39m.\u001b[39misinf(hidden_states)\u001b[39m.\u001b[39many():\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py:308\u001b[0m, in \u001b[0;36mT5LayerFF.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=305'>306</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states):\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=306'>307</a>\u001b[0m     forwarded_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=307'>308</a>\u001b[0m     forwarded_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mDenseReluDense(forwarded_states)\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=308'>309</a>\u001b[0m     hidden_states \u001b[39m=\u001b[39m hidden_states \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(forwarded_states)\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=309'>310</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py:269\u001b[0m, in \u001b[0;36mT5DenseReluDense.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=266'>267</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mrelu(hidden_states)\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=267'>268</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=268'>269</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwo(hidden_states)\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=269'>270</a>\u001b[0m \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/linear.py?line=101'>102</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/linear.py?line=102'>103</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/functional.py:1848\u001b[0m, in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/functional.py?line=1845'>1846</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight, bias):\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/functional.py?line=1846'>1847</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[39minput\u001b[39m, weight, bias), \u001b[39minput\u001b[39m, weight, bias\u001b[39m=\u001b[39mbias)\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/functional.py?line=1847'>1848</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, weight, bias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import T5ForConditionalGeneration\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\") \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\", use_fast=True)\n",
    "\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.train()\n",
    "for batch, (\n",
    "    token_type_ids, \n",
    "    attention_mask, \n",
    "    input_ids, \n",
    "    target_ids\n",
    ") in enumerate(train_loader):\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    token_type_ids = token_type_ids.to(device).long()\n",
    "    attention_mask = attention_mask.to(device).long()\n",
    "    input_ids = input_ids.to(device).long()\n",
    "    target_ids = target_ids.to(device).long()\n",
    "    \n",
    "    # Set the target ids and labels \n",
    "    y_ids = target_ids[:, :-1].contiguous()\n",
    "    lm_labels = target_ids[:, 1:].clone().detach()\n",
    "    \n",
    "    # Make sure that lm_labels are not set for 0 (pad) tokens\n",
    "    lm_labels[target_ids[:, 1:] == tokenizer.pad_token_id] = -100 \n",
    "    \n",
    "    \n",
    "    outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        decoder_input_ids=y_ids,\n",
    "        labels=lm_labels,\n",
    "    )\n",
    "    # print shape of outputs\n",
    "    outputs_copy = outputs \n",
    "    loss = outputs[0]\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    3,     2,     7,  3155,    71,   568, 13205,    16,     3,     9,\n",
      "         3270,    28,  3652,    11,     3,     9,     3, 18458,    36,    15,\n",
      "         5223,    12,    34,     6,    19,  9759,     3,     9,  1871, 13593,\n",
      "           49,   323,     8,  2815,     5,     1,    71,   568,  1067,  9759,\n",
      "            3,     9, 13593,    49,     5,     1,     1,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "['<pad><extra_id_0> A person in a dress with flowers and a stuffed bee attached to it, is pushing a stroller down the street.</s>']\n",
      "<pad><extra_id_0> A person in a dress with flowers and a stuffed bee attached to it, is pushing a stroller down the street.</s>\n"
     ]
    }
   ],
   "source": [
    "# Load t5_model.pt form local directory\n",
    "from torch.optim import AdamW\n",
    "from transformers import T5ForConditionalGeneration\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "# import pad_sequence\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "model.load_state_dict(torch.load(\"t5_model.pt\"))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\", use_fast=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "input_ids_list = []\n",
    "attention_mask_list = []\n",
    "\n",
    "temp_encoding = tokenizer.encode_plus('<s> A person dressed in a dress with flowers and a stuffed bee attached to it, is pushing a baby stroller down the street. </s> A person outside pushing a stroller.</s> ', \n",
    "            max_length=128,\n",
    "            pad_to_max_length=True,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            \n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "\n",
    "input_ids = torch.Tensor(temp_encoding.input_ids)\n",
    "attention_mask = torch.Tensor(temp_encoding.attention_mask)\n",
    "\n",
    "input_ids = input_ids.to(device).long()\n",
    "attention_mask = attention_mask.to(device).long()\n",
    "\n",
    "# input_ids_list.append(input_ids)\n",
    "# attention_mask_list.append(attention_mask)\n",
    "# \n",
    "# input_ids_list = pad_sequence(input_ids_list, batch_first=True)\n",
    "# attention_mask_list = pad_sequence(attention_mask_list, batch_first=True)\n",
    "# print(input_ids_list)\n",
    "# print(type(input_ids_list[0]))\n",
    "print(input_ids)\n",
    "\n",
    "# Set input_ids to be of shape (batch_size, seq_len)\n",
    "input_ids = input_ids.unsqueeze(0) \n",
    "attention_mask = attention_mask.unsqueeze(0)\n",
    "\n",
    "# create new list of inputs and attention_mask\n",
    "input_ids_list = []\n",
    "attention_mask_list = [] \n",
    "\n",
    "model.eval()\n",
    "output = model.generate(\n",
    "    input_ids = input_ids,\n",
    "    attention_mask = attention_mask,\n",
    "    num_beams = 7,\n",
    "    max_length = 100,\n",
    "    repetition_penalty = 2.0,\n",
    "    length_penalty = 0,\n",
    "    early_stopping = False,\n",
    "    use_cache = True,\n",
    "    )\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print([tokenizer.decode(g, skip_special_tokens=False, clean_up_tokenization_spaces=True) for g in output])\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=False, clean_up_tokenization_spaces=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generated_ids = model.generate(\n",
    "                input_ids = input_ids,\n",
    "                attention_mask = attention_mask,\n",
    "                num_beams = 10,\n",
    "                max_length = 100,\n",
    "                repetition_penalty = 1.0,\n",
    "                length_penalty = 1.0,\n",
    "                early_stopping = True,\n",
    "                use_cache = True,\n",
    "                )\n",
    "\n",
    "preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset esnli (/Users/julianbruinsma/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc)\n",
      "100%|██████████| 3/3 [00:00<00:00, 62.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of data: 569033\n",
      "Size of train data: 549367\n",
      "Size of val data: 9842\n",
      "Size of test data: 9824\n",
      " \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/julianbruinsma/stack/School/Master/Year_2/Natural Language Processing/Final Project/NLP-Final_Project/exploratory_analysis.ipynb Cell 12'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/exploratory_analysis.ipynb#ch0000010?line=14'>15</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mt5-base\u001b[39m\u001b[39m\"\u001b[39m, use_fast\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/exploratory_analysis.ipynb#ch0000010?line=16'>17</a>\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/exploratory_analysis.ipynb#ch0000010?line=18'>19</a>\u001b[0m dataset \u001b[39m=\u001b[39m esnli()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/exploratory_analysis.ipynb#ch0000010?line=20'>21</a>\u001b[0m \u001b[39m# load test loader \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/exploratory_analysis.ipynb#ch0000010?line=21'>22</a>\u001b[0m train_loader, val_loader, test_loader \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mget_data_loaders()\n",
      "File \u001b[0;32m~/stack/School/Master/Year_2/Natural Language Processing/Final Project/NLP-Final_Project/dataloader.py:45\u001b[0m, in \u001b[0;36mesnli.__init__\u001b[0;34m(self, frac_of_data)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/dataloader.py?line=42'>43</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval_data \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/dataloader.py?line=43'>44</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_data \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> <a href='file:///Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/dataloader.py?line=44'>45</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minitialize_data()\n",
      "File \u001b[0;32m~/stack/School/Master/Year_2/Natural Language Processing/Final Project/NLP-Final_Project/dataloader.py:51\u001b[0m, in \u001b[0;36mesnli.initialize_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/dataloader.py?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minitialize_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     <a href='file:///Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/dataloader.py?line=47'>48</a>\u001b[0m     \u001b[39m\"\"\" Initialize the data.\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/dataloader.py?line=48'>49</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='file:///Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/dataloader.py?line=50'>51</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload_data(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_df)\n\u001b[1;32m     <a href='file:///Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/dataloader.py?line=51'>52</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_data(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval_df)\n\u001b[1;32m     <a href='file:///Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/dataloader.py?line=52'>53</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_data(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_df)\n",
      "File \u001b[0;32m~/stack/School/Master/Year_2/Natural Language Processing/Final Project/NLP-Final_Project/dataloader.py:128\u001b[0m, in \u001b[0;36mesnli.load_data\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/dataloader.py?line=123'>124</a>\u001b[0m label_explanation \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlabel\u001b[39m}\u001b[39;00m\u001b[39m </s> \u001b[39m\u001b[39m{\u001b[39;00mexplanation\u001b[39m}\u001b[39;00m\u001b[39m </s>\u001b[39m\u001b[39m\"\u001b[39m \n\u001b[1;32m    <a href='file:///Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/dataloader.py?line=126'>127</a>\u001b[0m \u001b[39m# Tokenize the premise and hypothesis\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/dataloader.py?line=127'>128</a>\u001b[0m hypothesis_premise_tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mencode_plus(\n\u001b[1;32m    <a href='file:///Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/dataloader.py?line=128'>129</a>\u001b[0m     premise_hypothesis,\n\u001b[1;32m    <a href='file:///Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/dataloader.py?line=129'>130</a>\u001b[0m     truncation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, \n\u001b[1;32m    <a href='file:///Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/dataloader.py?line=130'>131</a>\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, \n\u001b[1;32m    <a href='file:///Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/dataloader.py?line=131'>132</a>\u001b[0m     max_length\u001b[39m=\u001b[39;49m\u001b[39m256\u001b[39;49m,\n\u001b[1;32m    <a href='file:///Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/dataloader.py?line=132'>133</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/dataloader.py?line=134'>135</a>\u001b[0m \u001b[39m# Tokenize the target explanation\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/dataloader.py?line=135'>136</a>\u001b[0m target_encoding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mencode_plus(\n\u001b[1;32m    <a href='file:///Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/dataloader.py?line=136'>137</a>\u001b[0m     label_explanation,\n\u001b[1;32m    <a href='file:///Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/dataloader.py?line=137'>138</a>\u001b[0m     truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/dataloader.py?line=140'>141</a>\u001b[0m     max_length\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/dataloader.py?line=141'>142</a>\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2536\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2525'>2526</a>\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2526'>2527</a>\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2527'>2528</a>\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2528'>2529</a>\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2532'>2533</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2533'>2534</a>\u001b[0m )\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2535'>2536</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_encode_plus(\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2536'>2537</a>\u001b[0m     text\u001b[39m=\u001b[39;49mtext,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2537'>2538</a>\u001b[0m     text_pair\u001b[39m=\u001b[39;49mtext_pair,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2538'>2539</a>\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2539'>2540</a>\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2540'>2541</a>\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2541'>2542</a>\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2542'>2543</a>\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2543'>2544</a>\u001b[0m     is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2544'>2545</a>\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2545'>2546</a>\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2546'>2547</a>\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2547'>2548</a>\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2548'>2549</a>\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2549'>2550</a>\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2550'>2551</a>\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2551'>2552</a>\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2552'>2553</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2553'>2554</a>\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2554'>2555</a>\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py:497\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=473'>474</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_encode_plus\u001b[39m(\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=474'>475</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=475'>476</a>\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=492'>493</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=493'>494</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BatchEncoding:\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=495'>496</a>\u001b[0m     batched_input \u001b[39m=\u001b[39m [(text, text_pair)] \u001b[39mif\u001b[39;00m text_pair \u001b[39melse\u001b[39;00m [text]\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=496'>497</a>\u001b[0m     batched_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_encode_plus(\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=497'>498</a>\u001b[0m         batched_input,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=498'>499</a>\u001b[0m         is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=499'>500</a>\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=500'>501</a>\u001b[0m         padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=501'>502</a>\u001b[0m         truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=502'>503</a>\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=503'>504</a>\u001b[0m         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=504'>505</a>\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=505'>506</a>\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=506'>507</a>\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=507'>508</a>\u001b[0m         return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=508'>509</a>\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=509'>510</a>\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=510'>511</a>\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=511'>512</a>\u001b[0m         return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=512'>513</a>\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=513'>514</a>\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=514'>515</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=516'>517</a>\u001b[0m     \u001b[39m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=517'>518</a>\u001b[0m     \u001b[39m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=518'>519</a>\u001b[0m     \u001b[39mif\u001b[39;00m return_tensors \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m return_overflowing_tokens:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py:472\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=469'>470</a>\u001b[0m \u001b[39mfor\u001b[39;00m input_ids \u001b[39min\u001b[39;00m sanitized_tokens[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=470'>471</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eventual_warn_about_too_long_sequence(input_ids, max_length, verbose)\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=471'>472</a>\u001b[0m \u001b[39mreturn\u001b[39;00m BatchEncoding(sanitized_tokens, sanitized_encodings, tensor_type\u001b[39m=\u001b[39;49mreturn_tensors)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:200\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=191'>192</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=192'>193</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=193'>194</a>\u001b[0m     data: Optional[Dict[\u001b[39mstr\u001b[39m, Any]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=197'>198</a>\u001b[0m     n_sequences: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=198'>199</a>\u001b[0m ):\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=199'>200</a>\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(data)\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=201'>202</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(encoding, EncodingFast):\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=202'>203</a>\u001b[0m         encoding \u001b[39m=\u001b[39m [encoding]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/collections/__init__.py:978\u001b[0m, in \u001b[0;36mUserDict.__init__\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/collections/__init__.py?line=974'>975</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/collections/__init__.py?line=975'>976</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdescriptor \u001b[39m\u001b[39m'\u001b[39m\u001b[39m__init__\u001b[39m\u001b[39m'\u001b[39m\u001b[39m of \u001b[39m\u001b[39m'\u001b[39m\u001b[39mUserDict\u001b[39m\u001b[39m'\u001b[39m\u001b[39m object \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/collections/__init__.py?line=976'>977</a>\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mneeds an argument\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/collections/__init__.py?line=977'>978</a>\u001b[0m \u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs \u001b[39m=\u001b[39m args\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/collections/__init__.py?line=978'>979</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/collections/__init__.py?line=979'>980</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mexpected at most 1 arguments, got \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m \u001b[39mlen\u001b[39m(args))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load t5_model.pt form local directory\n",
    "from torch.optim import AdamW\n",
    "from transformers import T5ForConditionalGeneration\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "# import pad_sequence\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from dataloader import esnli\n",
    "\n",
    "import t5_trainer\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "model.load_state_dict(torch.load(\"t5_model.pt\"))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\", use_fast=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "dataset = esnli()\n",
    "\n",
    "# load test loader \n",
    "train_loader, val_loader, test_loader = dataset.get_data_loaders()\n",
    "\n",
    "predictions, ground_truths = t5_trainer.evaluate(model, test_loader, tokenizer, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', '', '', '', '', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "predictions, ground_truths = t5_trainer.evaluate(model, test_loader, tokenizer, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0adcc2737ebf6a4a119f135174df96668767fca1ef1112612db5ecadf2b6d608"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
