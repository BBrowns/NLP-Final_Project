{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GmJRAgXP8pDN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/OpenSSL/crypto.py:14: CryptographyDeprecationWarning: Python 2 is no longer supported by the Python core team. Support for it is now deprecated in cryptography, and will be removed in the next release.\n",
            "  from cryptography import utils, x509\n",
            "\u001b[33mDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7.\u001b[0m\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/53/b4867d15b0023d43cce2c4f6e7f8b67487b99b43599127868a95da0e1f47/transformers-2.3.0.tar.gz (386kB)\n",
            "\u001b[K     |████████████████████████████████| 389kB 8.9MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: numpy in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from transformers) (1.8.0rc1)\n",
            "Collecting boto3 (from transformers)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/fd/d814f9cbefebbea88977628d11b860b5d564ba6f16f64c378e2da2a36405/boto3-1.17.112-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 28.9MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: requests in /Users/julianbruinsma/Library/Python/2.7/lib/python/site-packages (from transformers) (2.24.0)\n",
            "Requirement already satisfied: tqdm in /Users/julianbruinsma/Library/Python/2.7/lib/python/site-packages (from transformers) (4.48.2)\n",
            "Collecting regex!=2019.12.17 (from transformers)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/75/b5b60055897d78882da8bc4c94609067cf531a42726df2e44ce69e8ec7a9/regex-2022.1.18.tar.gz (382kB)\n",
            "\u001b[K     |████████████████████████████████| 389kB 27.5MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting sentencepiece (from transformers)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/aa/71/bb7d64dcd80a6506146397bca7310d5a8684f0f9ef035f03affb657f1aec/sentencepiece-0.1.96.tar.gz (508kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 28.8MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting sacremoses (from transformers)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7f/fe/ab4948aae0a47a6e29b7526de1e256b7d0dd1f0f45e9a71559c298196ff1/sacremoses-0.0.49.tar.gz (880kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 23.7MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting s3transfer<0.5.0,>=0.4.0 (from boto3->transformers)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/d0/693477c688348654ddc21dcdce0817653a294aa43f41771084c25e7ff9c7/s3transfer-0.4.2-py2.py3-none-any.whl (79kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 16.6MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting botocore<1.21.0,>=1.20.112 (from boto3->transformers)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c7/ea/11c3beca131920f552602b98d7ba9fc5b46bee6a59cbd48a95a85cbb8f41/botocore-1.20.112-py2.py3-none-any.whl (7.7MB)\n",
            "\u001b[K     |████████████████████████████████| 7.7MB 17.4MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1 (from boto3->transformers)\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: idna<3,>=2.5 in /Users/julianbruinsma/Library/Python/2.7/lib/python/site-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/julianbruinsma/Library/Python/2.7/lib/python/site-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/julianbruinsma/Library/Python/2.7/lib/python/site-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/julianbruinsma/Library/Python/2.7/lib/python/site-packages (from requests->transformers) (1.25.10)\n",
            "Requirement already satisfied: six in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /Users/julianbruinsma/Library/Python/2.7/lib/python/site-packages (from sacremoses->transformers) (7.1.2)\n",
            "Collecting joblib (from sacremoses->transformers)\n",
            "  Using cached https://files.pythonhosted.org/packages/28/5c/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf/joblib-0.14.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: futures<4.0.0,>=2.2.0; python_version == \"2.7\" in /Users/julianbruinsma/Library/Python/2.7/lib/python/site-packages (from s3transfer<0.5.0,>=0.4.0->boto3->transformers) (3.3.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/julianbruinsma/Library/Python/2.7/lib/python/site-packages (from botocore<1.21.0,>=1.20.112->boto3->transformers) (2.8.1)\n",
            "Building wheels for collected packages: transformers, regex, sentencepiece, sacremoses\n",
            "  Building wheel for transformers (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /Users/julianbruinsma/Library/Caches/pip/wheels/0a/0e/c6/57d97fd4f123e26973b95011a04ea828ebcdd9f5abcc645d56\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Complete output from command /System/Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python -u -c 'import setuptools, tokenize;__file__='\"'\"'/private/var/folders/b4/y8gt37ps1q9dq8sk0c8bv9_w0000gn/T/pip-install-A4Bids/regex/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /private/var/folders/b4/y8gt37ps1q9dq8sk0c8bv9_w0000gn/T/pip-wheel-uPUQal --python-tag cp27:\u001b[0m\n",
            "\u001b[31m  ERROR: running bdist_wheel\n",
            "  running build\n",
            "  running build_py\n",
            "  creating build\n",
            "  creating build/lib.macosx-12.2-x86_64-2.7\n",
            "  creating build/lib.macosx-12.2-x86_64-2.7/regex\n",
            "  copying regex_3/__init__.py -> build/lib.macosx-12.2-x86_64-2.7/regex\n",
            "  copying regex_3/regex.py -> build/lib.macosx-12.2-x86_64-2.7/regex\n",
            "  copying regex_3/_regex_core.py -> build/lib.macosx-12.2-x86_64-2.7/regex\n",
            "  copying regex_3/test_regex.py -> build/lib.macosx-12.2-x86_64-2.7/regex\n",
            "  running build_ext\n",
            "  building 'regex._regex' extension\n",
            "  creating build/temp.macosx-12.2-x86_64-2.7\n",
            "  creating build/temp.macosx-12.2-x86_64-2.7/regex_3\n",
            "  cc -fno-strict-aliasing -fno-common -dynamic -g -Os -pipe -fno-common -fno-strict-aliasing -fwrapv -DENABLE_DTRACE -DMACOSX -DNDEBUG -Wall -Wstrict-prototypes -Wshorten-64-to-32 -iwithsysroot /usr/local/libressl/include -DNDEBUG -g -fwrapv -Os -Wall -Wstrict-prototypes -DENABLE_DTRACE -pipe -I/System/Library/Frameworks/Python.framework/Versions/2.7/include/python2.7 -c regex_3/_regex.c -o build/temp.macosx-12.2-x86_64-2.7/regex_3/_regex.o\n",
            "  regex_3/_regex.c:755:23: error: expected expression\n",
            "      return *((Py_UCS1*)text + pos);\n",
            "                        ^\n",
            "  regex_3/_regex.c:755:15: error: use of undeclared identifier 'Py_UCS1'\n",
            "      return *((Py_UCS1*)text + pos);\n",
            "                ^\n",
            "  regex_3/_regex.c:760:16: error: expected expression\n",
            "      *((Py_UCS1*)text + pos) = (Py_UCS1)ch;\n",
            "                 ^\n",
            "  regex_3/_regex.c:760:8: error: use of undeclared identifier 'Py_UCS1'\n",
            "      *((Py_UCS1*)text + pos) = (Py_UCS1)ch;\n",
            "         ^\n",
            "  regex_3/_regex.c:760:32: error: use of undeclared identifier 'Py_UCS1'\n",
            "      *((Py_UCS1*)text + pos) = (Py_UCS1)ch;\n",
            "                                 ^\n",
            "  regex_3/_regex.c:765:21: error: expected expression\n",
            "      return (Py_UCS1*)text + pos;\n",
            "                      ^\n",
            "  regex_3/_regex.c:765:13: error: use of undeclared identifier 'Py_UCS1'\n",
            "      return (Py_UCS1*)text + pos;\n",
            "              ^\n",
            "  regex_3/_regex.c:770:23: error: expected expression\n",
            "      return *((Py_UCS2*)text + pos);\n",
            "                        ^\n",
            "  regex_3/_regex.c:770:15: error: use of undeclared identifier 'Py_UCS2'\n",
            "      return *((Py_UCS2*)text + pos);\n",
            "                ^\n",
            "  regex_3/_regex.c:775:16: error: expected expression\n",
            "      *((Py_UCS2*)text + pos) = (Py_UCS2)ch;\n",
            "                 ^\n",
            "  regex_3/_regex.c:775:8: error: use of undeclared identifier 'Py_UCS2'\n",
            "      *((Py_UCS2*)text + pos) = (Py_UCS2)ch;\n",
            "         ^\n",
            "  regex_3/_regex.c:775:32: error: use of undeclared identifier 'Py_UCS2'\n",
            "      *((Py_UCS2*)text + pos) = (Py_UCS2)ch;\n",
            "                                 ^\n",
            "  regex_3/_regex.c:780:21: error: expected expression\n",
            "      return (Py_UCS2*)text + pos;\n",
            "                      ^\n",
            "  regex_3/_regex.c:780:13: error: use of undeclared identifier 'Py_UCS2'\n",
            "      return (Py_UCS2*)text + pos;\n",
            "              ^\n",
            "  regex_3/_regex.c:2103:25: error: use of undeclared identifier 'PyExc_TimeoutError'; did you mean 'PyExc_ImportError'?\n",
            "          PyErr_SetString(PyExc_TimeoutError, \"regex timed out\");\n",
            "                          ^~~~~~~~~~~~~~~~~~\n",
            "                          PyExc_ImportError\n",
            "  /System/Library/Frameworks/Python.framework/Versions/2.7/include/python2.7/pyerrors.h:134:24: note: 'PyExc_ImportError' declared here\n",
            "  PyAPI_DATA(PyObject *) PyExc_ImportError;\n",
            "                         ^\n",
            "  regex_3/_regex.c:3468:9: error: unknown type name 'Py_UCS1'; did you mean 'Py_UCS4'?\n",
            "          Py_UCS1* text_ptr;\n",
            "          ^~~~~~~\n",
            "          Py_UCS4\n",
            "  /System/Library/Frameworks/Python.framework/Versions/2.7/include/python2.7/unicodeobject.h:128:22: note: 'Py_UCS4' declared here\n",
            "  typedef unsigned int Py_UCS4;\n",
            "                       ^\n",
            "  regex_3/_regex.c:3469:9: error: unknown type name 'Py_UCS1'; did you mean 'Py_UCS4'?\n",
            "          Py_UCS1* limit_ptr;\n",
            "          ^~~~~~~\n",
            "          Py_UCS4\n",
            "  /System/Library/Frameworks/Python.framework/Versions/2.7/include/python2.7/unicodeobject.h:128:22: note: 'Py_UCS4' declared here\n",
            "  typedef unsigned int Py_UCS4;\n",
            "                       ^\n",
            "  regex_3/_regex.c:3471:29: error: expected expression\n",
            "          text_ptr = (Py_UCS1*)text + text_pos;\n",
            "                              ^\n",
            "  regex_3/_regex.c:3471:21: error: use of undeclared identifier 'Py_UCS1'\n",
            "          text_ptr = (Py_UCS1*)text + text_pos;\n",
            "                      ^\n",
            "  fatal error: too many errors emitted, stopping now [-ferror-limit=]\n",
            "  20 errors generated.\n",
            "  error: command 'cc' failed with exit status 1\n",
            "  ----------------------------------------\u001b[0m\n",
            "\u001b[31m  ERROR: Failed building wheel for regex\u001b[0m\n",
            "\u001b[?25h  Running setup.py clean for regex\n",
            "  Building wheel for sentencepiece (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /Users/julianbruinsma/Library/Caches/pip/wheels/e8/51/c2/02ae1129f6dc38bf6b3a9efeabe926ac928ce8d5f5f458db26\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /Users/julianbruinsma/Library/Caches/pip/wheels/20/2f/e6/66df1cdb4ce675fa6ad271c8195f1cb9bcfb90cd7c9e1d97b5\n",
            "Successfully built transformers sentencepiece sacremoses\n",
            "Failed to build regex\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3, regex, sentencepiece, joblib, sacremoses, transformers\n",
            "\u001b[31mERROR: Could not install packages due to an EnvironmentError: [Errno 13] Permission denied: '/Library/Python/2.7/site-packages/jmespath-0.10.0.dist-info'\n",
            "Consider using the `--user` option or check the permissions.\n",
            "\u001b[0m\n",
            "/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/OpenSSL/crypto.py:14: CryptographyDeprecationWarning: Python 2 is no longer supported by the Python core team. Support for it is now deprecated in cryptography, and will be removed in the next release.\n",
            "  from cryptography import utils, x509\n",
            "\u001b[33mDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7.\u001b[0m\n",
            "Collecting datasets\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2d/2f/088475172a21d1e45e29088460feeec1ec4a01608c0758c81dd0ccb0246b/datasets-2.0.0.tar.gz (21.4MB)\n",
            "\u001b[K     |████████████████████████████████| 21.4MB 34.6MB/s eta 0:00:01\n",
            "\u001b[?25h\u001b[31m    ERROR: Complete output from command python setup.py egg_info:\u001b[0m\n",
            "\u001b[31m    ERROR: Traceback (most recent call last):\n",
            "      File \"<string>\", line 1, in <module>\n",
            "      File \"/private/var/folders/b4/y8gt37ps1q9dq8sk0c8bv9_w0000gn/T/pip-install-EhdILx/datasets/setup.py\", line 212, in <module>\n",
            "        long_description=open(\"README.md\", encoding=\"utf-8\").read(),\n",
            "    TypeError: 'encoding' is an invalid keyword argument for this function\n",
            "    ----------------------------------------\u001b[0m\n",
            "\u001b[31mERROR: Command \"python setup.py egg_info\" failed with error code 1 in /private/var/folders/b4/y8gt37ps1q9dq8sk0c8bv9_w0000gn/T/pip-install-EhdILx/datasets/\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OE_M1ydF9Ckm"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Reusing dataset esnli (/Users/julianbruinsma/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc)\n",
            "100%|██████████| 3/3 [00:00<00:00, 63.47it/s]\n",
            "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/auto/modeling_auto.py:803: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  warnings.warn(\n",
            "Downloading: 100%|██████████| 850M/850M [00:21<00:00, 41.1MB/s] \n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
        "import torch\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"esnli\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
        "model = AutoModelWithLMHead.from_pretrained(\"t5-base\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4wGrq_Is_hUe"
      },
      "outputs": [],
      "source": [
        "max_source_length = len(max(dataset['train']['premise'], key=len)) + len(max(dataset['train']['hypothesis'], key=len))\n",
        "max_target_length = 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "i9EGZ-cU_qRQ"
      },
      "outputs": [],
      "source": [
        "train = dataset['train']\n",
        "input_seqs = []\n",
        "output_seqs = []\n",
        "cnt = 0\n",
        "for row in train:\n",
        "    if cnt > 100000:\n",
        "        break\n",
        "    input_seqs.append(f\"explain: premise: {row['premise']} hypothesis: {row['hypothesis']}</s>\")\n",
        "    output_seqs.append(row['explanation_1'])\n",
        "    cnt += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTgwlDbaDVL3",
        "outputId": "e578158c-cad8-41e3-a610-6e1aa6002898"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/Users/julianbruinsma/stack/School/Master/Year_2/Natural Language Processing/Final Project/NLP-Final_Project/t5_explanation_generation.ipynb Cell 5'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/t5_explanation_generation.ipynb#ch0000004?line=0'>1</a>\u001b[0m encoding \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39;49mencode(input_seqs, padding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mlongest\u001b[39;49m\u001b[39m'\u001b[39;49m, max_length\u001b[39m=\u001b[39;49mmax_source_length, truncation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, return_tensors\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/t5_explanation_generation.ipynb#ch0000004?line=1'>2</a>\u001b[0m input_ids, attention_mask \u001b[39m=\u001b[39m encoding\u001b[39m.\u001b[39minput_ids, encoding\u001b[39m.\u001b[39mattention_mask\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/julianbruinsma/stack/School/Master/Year_2/Natural%20Language%20Processing/Final%20Project/NLP-Final_Project/t5_explanation_generation.ipynb#ch0000004?line=3'>4</a>\u001b[0m target_encoding \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mencode(output_seqs, padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlongest\u001b[39m\u001b[39m'\u001b[39m, max_length\u001b[39m=\u001b[39mmax_target_length, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2206\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2168'>2169</a>\u001b[0m \u001b[39m@add_end_docstrings\u001b[39m(\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2169'>2170</a>\u001b[0m     ENCODE_KWARGS_DOCSTRING,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2170'>2171</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2188'>2189</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2189'>2190</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mint\u001b[39m]:\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2190'>2191</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2191'>2192</a>\u001b[0m \u001b[39m    Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2192'>2193</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2203'>2204</a>\u001b[0m \u001b[39m            method).\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2204'>2205</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2205'>2206</a>\u001b[0m     encoded_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode_plus(\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2206'>2207</a>\u001b[0m         text,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2207'>2208</a>\u001b[0m         text_pair\u001b[39m=\u001b[39;49mtext_pair,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2208'>2209</a>\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2209'>2210</a>\u001b[0m         padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2210'>2211</a>\u001b[0m         truncation\u001b[39m=\u001b[39;49mtruncation,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2211'>2212</a>\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2212'>2213</a>\u001b[0m         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2213'>2214</a>\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2214'>2215</a>\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2215'>2216</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2217'>2218</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m encoded_inputs[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2536\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2525'>2526</a>\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2526'>2527</a>\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2527'>2528</a>\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2528'>2529</a>\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2532'>2533</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2533'>2534</a>\u001b[0m )\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2535'>2536</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_encode_plus(\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2536'>2537</a>\u001b[0m     text\u001b[39m=\u001b[39;49mtext,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2537'>2538</a>\u001b[0m     text_pair\u001b[39m=\u001b[39;49mtext_pair,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2538'>2539</a>\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2539'>2540</a>\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2540'>2541</a>\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2541'>2542</a>\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2542'>2543</a>\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2543'>2544</a>\u001b[0m     is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2544'>2545</a>\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2545'>2546</a>\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2546'>2547</a>\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2547'>2548</a>\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2548'>2549</a>\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2549'>2550</a>\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2550'>2551</a>\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2551'>2552</a>\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2552'>2553</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2553'>2554</a>\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=2554'>2555</a>\u001b[0m )\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py:497\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=473'>474</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_encode_plus\u001b[39m(\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=474'>475</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=475'>476</a>\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=492'>493</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=493'>494</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BatchEncoding:\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=495'>496</a>\u001b[0m     batched_input \u001b[39m=\u001b[39m [(text, text_pair)] \u001b[39mif\u001b[39;00m text_pair \u001b[39melse\u001b[39;00m [text]\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=496'>497</a>\u001b[0m     batched_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_encode_plus(\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=497'>498</a>\u001b[0m         batched_input,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=498'>499</a>\u001b[0m         is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=499'>500</a>\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=500'>501</a>\u001b[0m         padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=501'>502</a>\u001b[0m         truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=502'>503</a>\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=503'>504</a>\u001b[0m         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=504'>505</a>\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=505'>506</a>\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=506'>507</a>\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=507'>508</a>\u001b[0m         return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=508'>509</a>\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=509'>510</a>\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=510'>511</a>\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=511'>512</a>\u001b[0m         return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=512'>513</a>\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=513'>514</a>\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=514'>515</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=516'>517</a>\u001b[0m     \u001b[39m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=517'>518</a>\u001b[0m     \u001b[39m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=518'>519</a>\u001b[0m     \u001b[39mif\u001b[39;00m return_tensors \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m return_overflowing_tokens:\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py:424\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=414'>415</a>\u001b[0m \u001b[39m# Set the truncation and padding strategy and restore the initial configuration\u001b[39;00m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=415'>416</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_truncation_and_padding(\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=416'>417</a>\u001b[0m     padding_strategy\u001b[39m=\u001b[39mpadding_strategy,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=417'>418</a>\u001b[0m     truncation_strategy\u001b[39m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=420'>421</a>\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=421'>422</a>\u001b[0m )\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=423'>424</a>\u001b[0m encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tokenizer\u001b[39m.\u001b[39;49mencode_batch(\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=424'>425</a>\u001b[0m     batch_text_or_text_pairs,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=425'>426</a>\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=426'>427</a>\u001b[0m     is_pretokenized\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=427'>428</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=429'>430</a>\u001b[0m \u001b[39m# Convert encoding to dict\u001b[39;00m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=430'>431</a>\u001b[0m \u001b[39m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=431'>432</a>\u001b[0m \u001b[39m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=432'>433</a>\u001b[0m \u001b[39m#                       List[EncodingFast]\u001b[39;00m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=433'>434</a>\u001b[0m \u001b[39m#                    ]\u001b[39;00m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=434'>435</a>\u001b[0m \u001b[39m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=435'>436</a>\u001b[0m tokens_and_encodings \u001b[39m=\u001b[39m [\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=436'>437</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_convert_encoding(\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=437'>438</a>\u001b[0m         encoding\u001b[39m=\u001b[39mencoding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=446'>447</a>\u001b[0m     \u001b[39mfor\u001b[39;00m encoding \u001b[39min\u001b[39;00m encodings\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=447'>448</a>\u001b[0m ]\n",
            "\u001b[0;31mTypeError\u001b[0m: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]"
          ]
        }
      ],
      "source": [
        "encoding = tokenizer(input_seqs, padding='longest', max_length=max_source_length, truncation=True, return_tensors='pt')\n",
        "input_ids, attention_mask = encoding.input_ids, encoding.attention_mask\n",
        "\n",
        "target_encoding = tokenizer.encode(output_seqs, padding='longest', max_length=max_target_length, truncation=True, return_tensors='pt')\n",
        "labels = target_encoding.input_ids\n",
        "labels = torch.tensor(labels)\n",
        "labels[labels == tokenizer.pad_token_id] = -100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "PToor4JhA1kd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "neutral\n"
          ]
        }
      ],
      "source": [
        "test = dataset['test'][0]\n",
        "input_seq = f\"explain: premise: {test['premise']} hypothesis: {test['hypothesis']}</s>\"\n",
        "test_input = tokenizer.encode(input_seq, return_tensors='pt')\n",
        "outputs = model.generate(test_input, num_beams=4, max_length=100, early_stopping=True)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "\n",
        "# Use model to predict a new string and print it \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "t5_explanation_generation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
