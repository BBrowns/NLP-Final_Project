{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "t5_explanation_generation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmJRAgXP8pDN"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
        "import torch\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"esnli\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
        "model = AutoModelWithLMHead.from_pretrained(\"t5-base\")\n"
      ],
      "metadata": {
        "id": "OE_M1ydF9Ckm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_source_length = len(max(dataset['train']['premise'], key=len)) + len(max(dataset['train']['hypothesis'], key=len))\n",
        "max_target_length = 512"
      ],
      "metadata": {
        "id": "4wGrq_Is_hUe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = dataset['train']\n",
        "input_seqs = []\n",
        "output_seqs = []\n",
        "cnt = 0\n",
        "for row in train:\n",
        "    if cnt > 100000:\n",
        "        break\n",
        "    input_seqs.append(f\"explain: premise: {row['premise']} hypothesis: {row['hypothesis']}</s>\")\n",
        "    output_seqs.append(row['explanation_1'])\n",
        "    cnt += 1"
      ],
      "metadata": {
        "id": "i9EGZ-cU_qRQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoding = tokenizer(input_seqs, padding='longest', max_length=max_source_length, truncation=True, return_tensors='pt')\n",
        "input_ids, attention_mask = encoding.input_ids, encoding.attention_mask\n",
        "\n",
        "target_encoding = tokenizer(output_seqs, padding='longest', max_length=max_target_length, truncation=True, return_tensors='pt')\n",
        "labels = target_encoding.input_ids\n",
        "labels = torch.tensor(labels)\n",
        "labels[labels == tokenizer.pad_token_id] = -100"
      ],
      "metadata": {
        "id": "DTgwlDbaDVL3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e578158c-cad8-41e3-a610-6e1aa6002898"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = dataset['test'][0]\n",
        "input_seq = f\"explain: premise: {test['premise']} hypothesis: {test['hypothesis']}</s>\"\n",
        "test_input = tokenizer(input_seq, return_tensors='pt').input_ids\n",
        "outputs = model.generate(test_input)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "PToor4JhA1kd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}